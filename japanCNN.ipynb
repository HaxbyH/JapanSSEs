{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import copy\n",
    "# import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from SeparableConv import SeparableConv1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'cpu'"
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarthquakeData(Dataset):\n",
    "    def __init__(self, h_path, d_path):\n",
    "        self.c_path = h_path + d_path\n",
    "        self.h_len = len(h_path)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.c_path)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path = self.c_path[idx]\n",
    "\n",
    "        if idx > self.h_len:\n",
    "            y = 1\n",
    "        else:\n",
    "            y = 0\n",
    "            \n",
    "        X = np.loadtxt(path, delimiter=',', dtype=str).astype(np.float32)\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "class1_paths = glob.glob(\"./128days/nonSSE/*.csv\")\n",
    "class2_paths = glob.glob(\"./128days/SSE/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = EarthquakeData(class1_paths, class2_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(30, 128)"
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2151 717 717\n",
      "2151 717 717\n"
     ]
    }
   ],
   "source": [
    "# Calculat size for each dataloader (60% training, 20% validation and testing)\n",
    "total_len = len(dataset)\n",
    "train_size = int(total_len * 0.6)\n",
    "val_size = int(total_len * 0.2)\n",
    "test_size = total_len - train_size - val_size\n",
    "\n",
    "train_data, val_data, test_data = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "print(train_size, val_size, test_size)\n",
    "\n",
    "batch_size = 1\n",
    "dataloader = DataLoader(train_data, batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_data, batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_data, batch_size, shuffle=False)\n",
    "print(len(dataloader), len(test_dataloader), len(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "https://github.com/s4rduk4r/eegnet_pytorch\n",
    "    EEGNet PyTorch implementation\n",
    "    Original implementation - https://github.com/vlawhern/arl-eegmodels\n",
    "    Original paper: https://iopscience.iop.org/article/10.1088/1741-2552/aace8c\n",
    "\n",
    "    ---\n",
    "    EEGNet Parameters:\n",
    "\n",
    "      nb_classes      : int, number of classes to classify\n",
    "      Chans           : number of channels in the EEG data\n",
    "      Samples         : sample frequency (Hz) in the EEG data\n",
    "      dropoutRate     : dropout fraction\n",
    "      kernLength      : length of temporal convolution in first layer. \n",
    "                        ARL recommends to set this parameter to be half of the sampling rate. \n",
    "                        For the SMR dataset in particular since the data was high-passed at 4Hz ARL used a kernel length of 32.\n",
    "      F1, F2          : number of temporal filters (F1) and number of pointwise\n",
    "                        filters (F2) to learn. Default: F1 = 8, F2 = F1 * D.\n",
    "      D               : number of spatial filters to learn within each temporal\n",
    "                        convolution. Default: D = 2\n",
    "'''\n",
    "\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, nb_classes: int, Chans: int = 64, Samples: int = 128,\n",
    "                 dropoutRate: float = 0.5, kernLength: int = 63,\n",
    "                 F1:int = 8, D:int = 2):\n",
    "        super().__init__()\n",
    "\n",
    "        F2 = F1 * D\n",
    "\n",
    "        # Make kernel size and odd number\n",
    "        try:\n",
    "            assert kernLength % 2 != 0\n",
    "        except AssertionError:\n",
    "            raise ValueError(\"ERROR: kernLength must be odd number\")\n",
    "\n",
    "        # In: (B, Chans, Samples, 1)\n",
    "        # Out: (B, F1, Samples, 1)\n",
    "        self.conv1 = nn.Conv1d(Chans, F1, kernLength, padding=(kernLength // 2))\n",
    "        self.bn1 = nn.BatchNorm1d(F1) # (B, F1, Samples, 1)\n",
    "        # In: (B, F1, Samples, 1)\n",
    "        # Out: (B, F2, Samples - Chans + 1, 1)\n",
    "        self.conv2 = nn.Conv1d(F1, F2, Chans, groups=F1)\n",
    "        self.bn2 = nn.BatchNorm1d(F2) # (B, F2, Samples - Chans + 1, 1)\n",
    "        # In: (B, F2, Samples - Chans + 1, 1)\n",
    "        # Out: (B, F2, (Samples - Chans + 1) / 4, 1)\n",
    "        self.avg_pool = nn.AvgPool1d(1)\n",
    "        self.dropout = nn.Dropout(dropoutRate)\n",
    "\n",
    "        # In: (B, F2, (Samples - Chans + 1) / 4, 1)\n",
    "        # Out: (B, F2, (Samples - Chans + 1) / 4, 1)\n",
    "        self.conv3 = SeparableConv1d(F2, F2, kernel_size=15, padding=7)\n",
    "        self.bn3 = nn.BatchNorm1d(F2)\n",
    "        # In: (B, F2, (Samples - Chans + 1) / 4, 1)\n",
    "        # Out: (B, F2, (Samples - Chans + 1) / 32, 1)\n",
    "        self.avg_pool2 = nn.AvgPool1d(1)\n",
    "        # In: (B, F2 *  (Samples - Chans + 1) / 32)\n",
    "        # 32x2960 (incomming) -> Outgoing = 1\n",
    "        # self.fc = nn.Linear(F2 * ((Samples - Chans + 1) // 32), nb_classes)\n",
    "        self.flatten = torch.flatten\n",
    "        # self.fc = nn.Linear(2960, nb_classes)\n",
    "        self.fc = nn.Linear(2992, nb_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # Block 1\n",
    "        print(\"BLOCK 1\")\n",
    "        print(\"-------------------------\")\n",
    "        print()\n",
    "        print(\"INPUT: \", x.shape)\n",
    "        y1 = self.conv1(x)\n",
    "        print(\"AFTER CONV1: \", y1.shape)\n",
    "        y1 = self.bn1(y1)\n",
    "        # print(\"AFTER BATCH1: \", y1.shape)\n",
    "        y1 = self.conv2(y1)\n",
    "        print(\"AFTER CONV2: \", y1.shape)\n",
    "        y1 = F.relu(self.bn2(y1))\n",
    "        print(\"AFTER RELU ACTIVATION: \", y1.shape)\n",
    "        y1 = self.avg_pool(y1)\n",
    "        print(\"AFTER AVEPOOL\", y1.shape)\n",
    "        y1 = self.dropout(y1)\n",
    "        # print(\"AFTER DROPOUT: \", y1.shape)\n",
    "        print()\n",
    "        print(\"BLOCK 2\")\n",
    "        print(\"-------------------------\")\n",
    "        print()\n",
    "\n",
    "        # Block 2\n",
    "        y2 = self.conv3(y1)\n",
    "        print(\"AFTER CONV3: \", y2.shape)\n",
    "        y2 = F.relu(self.bn3(y2))\n",
    "        print(\"AFTER RELU ACTIVATION: \", y2.shape)\n",
    "        y2 = self.avg_pool2(y2)\n",
    "        print(\"AFTER AVE2: \", y2.shape)\n",
    "        y2 = self.dropout(y2)\n",
    "        # print(\"dropout\", y2.shape)\n",
    "        y2 = self.flatten(y2, 1)\n",
    "        print(\"AFTER FLATTEN: \", y2.shape)\n",
    "        y2 = torch.sigmoid(self.fc(y2))\n",
    "        print(\"fc\", y2.shape)\n",
    "\n",
    "        return y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_chans = 30\n",
    "\n",
    "# if filter:\n",
    "#     num_chans = len(filter)\n",
    "\n",
    "model = EEGNet(Chans = num_chans, Samples=64, nb_classes=1, kernLength=7).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(y_pred, y_test):\n",
    "    # print(f\"Y pred: {y_pred}\")\n",
    "    # print(f\"y_test: {y_test}\")\n",
    "    prediction = torch.round(y_pred)\n",
    "    # print(f\"Prediction: {prediction}\")\n",
    "    correct_pred = (prediction == y_test).float()\n",
    "    acc = correct_pred.sum() / len(correct_pred)\n",
    "    acc = torch.round(acc * 100)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, dataloader, epochs):\n",
    "    start_epoch = 0\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    lowest_loss = 100\n",
    "\n",
    "    accuracy_stats = {\n",
    "        'train': [],\n",
    "        \"val\": []\n",
    "    }\n",
    "    loss_stats = {\n",
    "        'train': [],\n",
    "        \"val\": []\n",
    "    }\n",
    "\n",
    "    check_path = glob.glob(\"./checkpoints/*.tar\")\n",
    "    if check_path:\n",
    "        checkpoint = torch.load(check_path[0])\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        loss_stats = checkpoint['loss_stats']\n",
    "        accuracy_stats = checkpoint['accuracy_stats']\n",
    "        best_model_wts = checkpoint[\"best_wts\"]\n",
    "        lowest_loss = checkpoint[\"lowest_loss\"]\n",
    "        model.train()\n",
    "        print(f\"Found checkpoint. Epoch: {start_epoch-1} | Train acc: {accuracy_stats['train'][-1]} | Val acc: {accuracy_stats['val'][-1]}\")\n",
    "\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        # Training\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        data_size = 0\n",
    "        iteration = 0\n",
    "        for features, labels in dataloader:\n",
    "            if iteration % 100 == 0:\n",
    "                print(f\"Iteration: {iteration} / {len(dataloader)}\")\n",
    "            iteration += 1\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            features = features.float()\n",
    "            labels = labels.float()\n",
    "            print(features.shape)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            # print(features.shape)\n",
    "            # print(features)\n",
    "            # print(model)\n",
    "            y_pred = model(features)\n",
    "            y_pred = torch.squeeze(y_pred)\n",
    "            # print(y_pred)\n",
    "            # print(labels)\n",
    "            loss = criterion(y_pred, labels)\n",
    "            acc = binary_acc(y_pred, labels)\n",
    "\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            train_acc += acc.item()\n",
    "\n",
    "            # data_size += 1\n",
    "            # # if data_size % 100 == 0:\n",
    "            # print(f\"{data_size} / {len(dataloader)}\")\n",
    "        \n",
    "        # Validating\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            val_acc = 0\n",
    "            for val_features, val_labels in val_dataloader:\n",
    "                val_features, val_labels = val_features.to(device), val_labels.to(device)\n",
    "                # val_features = torch.unsqueeze(val_features, 1)\n",
    "                val_features = val_features.float()\n",
    "                val_labels = val_labels.float()\n",
    "\n",
    "\n",
    "                val_pred = model(val_features)\n",
    "                val_pred = torch.squeeze(val_pred)\n",
    "                val_loss_item = criterion(val_pred, val_labels)\n",
    "                val_acc_item = binary_acc(val_pred, val_labels)\n",
    "\n",
    "                val_loss += val_loss_item.item()\n",
    "                val_acc += val_acc_item.item()\n",
    "\n",
    "                if val_loss < lowest_loss:\n",
    "                    lowest_loss = val_loss  \n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "\n",
    "        loss_stats['train'].append(train_loss/len(dataloader))\n",
    "        loss_stats['val'].append(val_loss/len(val_dataloader))\n",
    "        accuracy_stats['train'].append(train_acc/len(dataloader))\n",
    "        accuracy_stats['val'].append(val_acc/len(val_dataloader))\n",
    "\n",
    "        \n",
    "        print(f'Epoch {epoch+0:03}: | Train Loss: {train_loss/len(dataloader):.5f} | Val Loss: {val_loss/len(val_dataloader):.5f} | Train Acc: {train_acc/len(dataloader):.3f} | Val Acc: {val_acc/len(val_dataloader):.3f}')\n",
    "        path = f\"./checkpoints/check_e.tar\"\n",
    "        torch.save({\n",
    "            \"epoch\": (epoch+1),\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"loss_stats\": loss_stats,\n",
    "            \"accuracy_stats\": accuracy_stats,\n",
    "            \"best_wts\": best_model_wts,\n",
    "            \"lowest_loss\": lowest_loss,\n",
    "        }, path)\n",
    "    return best_model_wts, accuracy_stats, loss_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 / 2151\n",
      "torch.Size([1, 30, 128])\n",
      "BLOCK 1\n",
      "-------------------------\n",
      "\n",
      "INPUT:  torch.Size([1, 30, 128])\n",
      "AFTER CONV1:  torch.Size([1, 8, 128])\n",
      "AFTER CONV2:  torch.Size([1, 16, 99])\n",
      "AFTER RELU ACTIVATION:  torch.Size([1, 16, 99])\n",
      "AFTER AVEPOOL torch.Size([1, 16, 99])\n",
      "\n",
      "BLOCK 2\n",
      "-------------------------\n",
      "\n",
      "AFTER CONV3:  torch.Size([1, 16, 99])\n",
      "AFTER RELU ACTIVATION:  torch.Size([1, 16, 99])\n",
      "AFTER AVE2:  torch.Size([1, 16, 99])\n",
      "AFTER FLATTEN:  torch.Size([1, 1584])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x1584 and 2992x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vr/sctwb2fd2vj9q1wdy6r6b29c1tmyjp/T/ipykernel_79953/758174941.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# print(dataloader)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbest_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_stats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/vr/sctwb2fd2vj9q1wdy6r6b29c1tmyjp/T/ipykernel_79953/3092400589.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, criterion, dataloader, epochs)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;31m# print(features)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;31m# print(model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;31m# print(y_pred)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch_venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/vr/sctwb2fd2vj9q1wdy6r6b29c1tmyjp/T/ipykernel_79953/1449325811.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AFTER FLATTEN: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch_venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch_venv/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x1584 and 2992x1)"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "# print(dataloader)\n",
    "best_weights, accuracy_stats, loss_stats = train(model, optimizer, criterion, dataloader, epochs=epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 64-bit ('pytorch_venv': conda)",
   "metadata": {
    "interpreter": {
     "hash": "791053d5088c9233e919eef0d93a291bda3986f6463af24513be9b0f42d44376"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}